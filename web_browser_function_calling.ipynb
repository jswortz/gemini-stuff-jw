{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Browser Function Calling with the Vertex AI Gemini API & Python SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upgrade the sdk for Gemini\n",
    "# !pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install playwright > /dev/null\n",
    "# !pip install  lxml\n",
    "\n",
    "# If this is your first time using playwright, you'll have to install a browser executable.\n",
    "# Running `playwright install` by default installs a chromium browser executable.\n",
    "# !playwright install\n",
    "# Also run this:\n",
    "# !playwright install dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restart current runtime\n",
    "\n",
    "To use the newly installed packages in this Jupyter runtime, it is recommended to restart the runtime. Run the following cell to restart the current kernel.\n",
    "\n",
    "The restart process might take a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be utilizing pre-built Langchain tools for the Playwright Browser\n",
    "\n",
    "[Guide here](https://python.langchain.com/docs/integrations/toolkits/playwright) - most of this code is from this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-14 23:37:53.529708: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-14 23:37:55.289169: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-12-14 23:37:55.289443: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-12-14 23:37:55.289456: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "from vertexai.preview.generative_models import (\n",
    "    Content,\n",
    "    FunctionDeclaration,\n",
    "    GenerativeModel,\n",
    "    Part,\n",
    "    Tool,\n",
    ")\n",
    "from langchain.agents.agent_toolkits import PlayWrightBrowserToolkit\n",
    "from langchain.tools.playwright.utils import (\n",
    "    create_async_playwright_browser,  # A synchronous browser is available, though it isn't compatible with jupyter.\n",
    ")\n",
    "from langchain.tools import Tool as LangchainTool\n",
    "# This import is required only for jupyter notebooks, since they have their own eventloop\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'click_element': ClickTool(async_browser=<Browser type=<BrowserType name=chromium executable_path=/home/jwortz_google_com/.cache/ms-playwright/chromium-1084/chrome-linux/chrome> version=119.0.6045.9>),\n",
       " 'navigate_browser': NavigateTool(async_browser=<Browser type=<BrowserType name=chromium executable_path=/home/jwortz_google_com/.cache/ms-playwright/chromium-1084/chrome-linux/chrome> version=119.0.6045.9>),\n",
       " 'previous_webpage': NavigateBackTool(async_browser=<Browser type=<BrowserType name=chromium executable_path=/home/jwortz_google_com/.cache/ms-playwright/chromium-1084/chrome-linux/chrome> version=119.0.6045.9>),\n",
       " 'extract_text': ExtractTextTool(async_browser=<Browser type=<BrowserType name=chromium executable_path=/home/jwortz_google_com/.cache/ms-playwright/chromium-1084/chrome-linux/chrome> version=119.0.6045.9>),\n",
       " 'extract_hyperlinks': ExtractHyperlinksTool(async_browser=<Browser type=<BrowserType name=chromium executable_path=/home/jwortz_google_com/.cache/ms-playwright/chromium-1084/chrome-linux/chrome> version=119.0.6045.9>),\n",
       " 'get_elements': GetElementsTool(async_browser=<Browser type=<BrowserType name=chromium executable_path=/home/jwortz_google_com/.cache/ms-playwright/chromium-1084/chrome-linux/chrome> version=119.0.6045.9>),\n",
       " 'current_webpage': CurrentWebPageTool(async_browser=<Browser type=<BrowserType name=chromium executable_path=/home/jwortz_google_com/.cache/ms-playwright/chromium-1084/chrome-linux/chrome> version=119.0.6045.9>)}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "async_browser = create_async_playwright_browser()\n",
    "toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
    "langchain_tools = toolkit.get_tools()\n",
    "tools_by_name = {tool.name: tool for tool in langchain_tools}\n",
    "tools_by_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the args schema to a `FunctionDeclaration`\n",
    "\n",
    "```python\n",
    "tools[0].args_schema.schema()\n",
    "\n",
    "{'title': 'ClickToolInput',\n",
    " 'description': 'Input for ClickTool.',\n",
    " 'type': 'object',\n",
    " 'properties': {'selector': {'title': 'Selector',\n",
    "   'description': 'CSS selector for the element to click',\n",
    "   'type': 'string'}},\n",
    " 'required': ['selector']}\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def convert_function_declartion(langchain_tool: LangchainTool) -> FunctionDeclaration:\n",
    "    \"\"\"Converts a LangchainTool to a FunctionDeclaration\n",
    "\n",
    "    Args:\n",
    "        langchain_tool (LangchainTool): The LangchainTool to convert\n",
    "\n",
    "    Returns:\n",
    "        FunctionDeclaration: The FunctionDeclaration representation of the LangchainTool\n",
    "    \"\"\"\n",
    "    langchain_args_schema = langchain_tool.args_schema.schema()\n",
    "    if 'title' in langchain_args_schema.keys():\n",
    "        langchain_args_schema.pop('title')\n",
    "    if 'description' in langchain_args_schema.keys():\n",
    "        description = langchain_args_schema.pop('description')\n",
    "    else:\n",
    "        description = langchain_tool.name\n",
    "    for property in langchain_args_schema['properties'].keys():\n",
    "        if 'title' in langchain_args_schema['properties'][property].keys():\n",
    "            langchain_args_schema['properties'][property].pop('title')\n",
    "        if 'default' in langchain_args_schema['properties'][property].keys():\n",
    "            langchain_args_schema['properties'][property].pop('default')\n",
    "        if 'type_' in  langchain_args_schema['properties'][property].keys():\n",
    "            langchain_args_schema['properties'][property]['type'] = langchain_args_schema['properties'][property].pop('type_')\n",
    "    function_declaration = FunctionDeclaration(\n",
    "        name=langchain_tool.name,\n",
    "        description=description,\n",
    "        parameters=langchain_args_schema\n",
    "    )\n",
    "    return(function_declaration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_tools = [convert_function_declartion(tool) for tool in langchain_tools]\n",
    "webbrowser_tools = Tool(gemini_tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Gemini Pro model\n",
    "\n",
    "The Gemini Pro (`gemini-pro`) model is designed to handle natural language tasks, multiturn text and code chat, and code generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidates {\n",
       "  content {\n",
       "    role: \"model\"\n",
       "    parts {\n",
       "      function_call {\n",
       "        name: \"navigate_browser\"\n",
       "        args {\n",
       "          fields {\n",
       "            key: \"url\"\n",
       "            value {\n",
       "              string_value: \"https://langchain.com/\"\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  finish_reason: STOP\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_HARASSMENT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_HATE_SPEECH\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "  safety_ratings {\n",
       "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
       "    probability: NEGLIGIBLE\n",
       "  }\n",
       "}\n",
       "usage_metadata {\n",
       "  prompt_token_count: 10\n",
       "  total_token_count: 10\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"What are the headers on langchain.com?\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config={\"temperature\": 0},\n",
    "    tools=[webbrowser_tools]\n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Quick inspection of one function call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"navigate_browser\"\n",
       "args {\n",
       "  fields {\n",
       "    key: \"url\"\n",
       "    value {\n",
       "      string_value: \"https://langchain.com/\"\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform_v1beta1.types.tool import FunctionCall\n",
    "from vertexai.generative_models._generative_models import GenerationResponse\n",
    "from typing import List\n",
    "\n",
    "async def use_browser_tools(part: Part, tools_by_name: Dict = tools_by_name):\n",
    "    \"\"\"\n",
    "    Run a tool from the browser.\n",
    "\n",
    "    Args:\n",
    "        part: The part to run.\n",
    "        tools_by_name: A dictionary of tool names to Langchain tools.\n",
    "\n",
    "    Returns:\n",
    "        A Part function response representing the results of the tool run.\n",
    "    \"\"\"\n",
    "\n",
    "    single_function_call = part.function_call\n",
    "    tool_name = single_function_call.name\n",
    "    single_function_args = single_function_call.args\n",
    "    if single_function_call.args is None:\n",
    "        tool_params = {}\n",
    "    else:\n",
    "        tool_params = {k: single_function_args[k] for k in single_function_args}\n",
    "    if tool_name != '':\n",
    "        response = await tools_by_name[tool_name].arun(tool_params)\n",
    "        return(Part.from_function_response(\n",
    "        name=tool_name,\n",
    "        response={\n",
    "            \"content\": {\"response\": response},\n",
    "        }\n",
    "        ))\n",
    "\n",
    "\n",
    "def get_parts_from_response(response: GenerationResponse) -> List[FunctionCall]:\n",
    "    \"\"\"\n",
    "    Get the parts from a generation response.\n",
    "\n",
    "    Args:\n",
    "        response: The generation response.\n",
    "\n",
    "    Returns:\n",
    "        A list of parts.\n",
    "    \"\"\"\n",
    "    return(response.candidates[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now do a mult-turn chat\n",
    "Example can be found here on [function calling](https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling#function-calling-one-and-a-half-turn-curl-sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeModel(\"gemini-pro\", \n",
    "                        generation_config={\"temperature\": 0},\n",
    "                        tools=[webbrowser_tools])\n",
    "chat = model.start_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"Yes, that\\'s correct. The h1 header on langchain.com is \\\"LangChain\\\", and the h2 headers are \\\"A complete set of powerful building blocks\\\", \\\"Smart connections to any source of data or knowledge\\\", \\\"Easily-swappable components\\\", \\\"A developer platform optimized for production-readiness\\\", \\\"One framework.\\\\nInfinite use-cases.\\\", \\\"Harness the power\\342\\200\\223and wrangle the complexity\\342\\200\\223 of LLMs\\\", \\\"Thanks to our contributors and partners for making LangChain awesome\\\", and \\\"Ready to build?\\\".\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 291\n",
      "  candidates_token_count: 109\n",
      "  total_token_count: 400\n",
      "}\n",
      "\n",
      "[None]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "response = chat.send_message(\"What are the headers (h1 and h2) on langchain.com?\")\n",
    "pprint(response)\n",
    "function_calls = get_parts_from_response(response)\n",
    "function_calls = function_calls.parts\n",
    "\n",
    "# function_calls\n",
    "\n",
    "tool_commands = [await use_browser_tools(fn_call) for fn_call in function_calls]\n",
    "print(tool_commands)\n",
    "if tool_commands[0] is None:\n",
    "    pass #using this as a generic way to stop when there are no tool responses - looking for better way TODO\n",
    "else:\n",
    "    pprint([chat.send_message(command) for command in tool_commands])    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
